{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raspagem múltipla entre páginas\n",
    "[](https://media.tenor.com/images/f1921d017450cd690cfa73bfe33d7724/tenor.gif)\n",
    "Nosso objetivo nesse Notebook será a raspagem de dados entre páginas. Nosso site continua sendo o [Quotes](http://quotes.toscrape.com/), queremos extrair a frase, o autor e a categoria de todas as frases que a página possui. No [Notebook anterior](https://github.com/DwarfThief/Raspagem-de-dados-para-iniciantes/blob/master/Raspagem%20multipla.ipynb) nos já conseguimos colher o texto de toda a página, mas... e as outras?\n",
    "\n",
    "O nosso programa deve ser capaz de acessar o site, colher toda a informação da página, \"clicar\" no botão `next ->` e acessar a próxima página, onde ele irá repetir todo o processo. Podemos ver facilmente que isso será um loop, mas para criarmos o Loop devemos entender como o botão funciona.\n",
    "\n",
    "## Inspecionando o botão \"next\"\n",
    "\n",
    "Ao inspecionar o botão podemo sperceber que ele faz parte da categoria `<li>` e sua classe é `next`, agora vamos tentar extrair o que tem no css desse botão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-16306e6609df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'li.next'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "response.css('li.next')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Html\n",
    "[<Selector xpath=\"descendant-or-self::li[@class and contains(concat(' ', normalize-space(@class), ' '), ' next ')]\" data='<li class=\"next\">\\n                <a hre'>]\n",
    "```\n",
    "A saída sera toda a lista de itens do objeto, mas se olhar detalhadamente para o código fonte, vamos ver que o link real está na subcategoria `<a>`, então vamos ver o que sai se extrairmos isso e a sua String junto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e6079651ee16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'li.next > a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "response.css('li.next > a').extract_first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que esse botão ao ser clicado nos envia para a próxima página atraves do `href`, então oq devemos é extrair esse link. Para isso vamos extrair a referência dentro do `href`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5c7e75b34f20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'li.next > a::atrr(href)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_first\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "proxima_pag = response.css('li.next > a::attr(href)').extract_first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A saída será uma String com o endereço para a próxima página, agora devemos unir essa parte com o resto do endereço que usamos no `start_urls`, então vamos usar o método `.urljoin()` e passar como parâmetro a nossa variável `proxima_pag`, assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5cf8ef9ed390>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxima_pag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "response.urljoin(proxima_pag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora nossa Spider extrair o endereço da próxima página, mas ainda não fizemos nada com essa informação, ta na hora de fazer com que a Spider vá para a próxima página. Precisamos guardar o novo URL em uma variável, podemos fazer, assim:\n",
    "```Python\n",
    "proxima_pag = response.css('li.next > a::attr(href)').extract_first()\n",
    "proxima_pag = response.urljoin(proxima_pag)\n",
    "```\n",
    "ou assim:\n",
    "\n",
    "```Python\n",
    "proxima_pag = response.urljoin(response.css('li.next > a::attr(href)').extract_first())\n",
    "```\n",
    "\n",
    "As duas formas dão na mesma coisa, eu usarei a 2ª opção. Continuando ando com nosso programa, vamos implementar esse código dentro da nossa Spider, o código ficaria assim:\n",
    "```Python\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com/random']\n",
    "\n",
    "    def parse(self, response):\n",
    "        #Extraindo as citações\n",
    "        for quote in response.css('div.quote'):     \n",
    "            caixa = {\n",
    "                'autor': quote.css('small.author::text').extract_first(),\n",
    "                'texto': quote.css('span.text::text').extract_first(),\n",
    "                'categorias': quote.css('a.tag::text').extract(),\n",
    "            }\n",
    "            yield caixa\n",
    "        #Navegação entre páginas\n",
    "        proxima_pag = response.urljoin(response.css('li.next > a::attr(href)').extract_first())\n",
    "```\n",
    "Perceba que o novo código não fica dentro do `for quote in response.css('div.quote'):`. Agora vamos usar um `yield` e usar o método `scrapy.Request()` do scrapy, precisamos enviar dois parâmetros, o primeiro é a url que vamos acessar, sendo assim, passamos nossa a variável `proxima_pag`. Como segundo parâmetro escrevemos `callback = self.parse`, assim ele repetirá o método `parse` que nos ja definimos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'yield' outside function (<ipython-input-6-bf247c8e7bfb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-bf247c8e7bfb>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    yield scrapy.Request( url = proxima_pag, callback = self.parse)\u001b[0m\n\u001b[1;37m                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'yield' outside function\n"
     ]
    }
   ],
   "source": [
    "yield scrapy.Request( url = proxima_pag, callback = self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código final\n",
    "```Python\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        #Extraindo as citações\n",
    "        for quote in response.css('div.quote'):     \n",
    "            caixa = {\n",
    "                'autor': quote.css('small.author::text').extract_first(),\n",
    "                'texto': quote.css('span.text::text').extract_first(),\n",
    "                'categorias': quote.css('a.tag::text').extract(),\n",
    "            }\n",
    "            yield caixa\n",
    "        #Navegação entre páginas\n",
    "        proxima_pag = response.urljoin(response.css('li.next > a::attr(href)').extract_first())\n",
    "        yield scrapy.Request( url = proxima_pag, callback = self.parse)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
