{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.creativecommons.org/l/by/4.0/88x31.png)\n",
    "# Primeira Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, vamos criar nossa Spider usando o comando `scrapy genspider quotes toscrape.com`, o parâmetro `quotes`é o nome da nossa aranha, normalmente nomeamos elas pelo nome do site, ja o segundo parâmetro é o domínio que ela visitará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy genspider quotes toscrape.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entendendo a Spider\n",
    "Criado a nossa spider, vamos editar o `quotes.py`. O código vai estar assim:\n",
    "```Python\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['toscrape.com']\n",
    "    start_urls = ['http://toscrape.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass\n",
    "\n",
    "```\n",
    "\n",
    "Como podemos ver, a aranha é do tipo classe classe e ela possui alguns atributos, vejamos...\n",
    "* `name`: É o nome que foi dado durante sua criação e é o nome que usaremos para ativa-lá.\n",
    "* `allowed_domains`: São os dominios que a aranha pode ir, usamos isso para certificar que a aranha não irá para domínios que não são permitidos (acessar um domínio errado pode gerar problemas com a lei, cuidado).\n",
    "* `start_urls`: É o url inical quando a aranha for executada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editando a Spider\n",
    "Vamos começar a editar nossa Spider, ela não veio perfeita... **Mãos a obra!**\n",
    "\n",
    "1. Mudando o `start_urls`\n",
    "    * Essa é rápida, devemos alterar o url inicial, visto que este não é o url que queremos, ele deve ficar assim:\n",
    "    `start_urls = ['http://quotes.toscrape.com/random']`\n",
    "2. Editando o `def parse(self, response):`\n",
    "    * Essa parte é importante, esse método é chamado toda vez que a aranha é executada, sendo assim, o que estiver dentro deste método será executado no seu início. Vamos testar isso fazendo com que nossa aranha nos mande uma mensagem durante sua invocação.\n",
    "    ```Python\n",
    "    def parse(self, response):\n",
    "        self.log(\"Eu estou viva e caminhei pelo \"+ response.url)\n",
    "    ```\n",
    "\n",
    "    * Agora rode sua aranha usando o comando:\n",
    "    \n",
    "    `scrapy runspider quotes.py`\n",
    "    \n",
    "    * Você vai perceber que vários textos serão impressos, no momento, abstraia e procure pela sua mensagem, ela deve estar por ai... Agora vamos dificultar as coisas, primeiro vamos criar um generator para a informação e declarar as informações que queremos guardar, dessa formar:\n",
    "     \n",
    "     ```Python\n",
    "     def parse(self, response):\n",
    "          yield{\n",
    "              'text':response.css('span.text::text').extract_first() ,\n",
    "              'autor':response.css('small.author::text').extract_first() ,\n",
    "              'categorias':response.css('a.tag::text').extract() , \n",
    "          }\n",
    "     ```\n",
    "     \n",
    "    * Agora vamos rodar a spider novamente:\n",
    "    \n",
    "    `scrapy runspider quotes.py`\n",
    "    \n",
    "    * Você vai notar que a Spider extraiu a informação do site, procure no meio da bagunça! Agora vamos salvar essa informação em um `.json`, é bem simples: `scrapy runspider quotes.py -o site.json`.\n",
    "    * Agora vamos ver o que a nossa aranha salvou no arquivo, para isso vamos rodar o comando: `more site.json`.\n",
    "\n",
    "### Então o código final deve estar assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com/random']\n",
    "\n",
    "    def parse(self, response):\n",
    "        yield{\n",
    "            'text':response.css('span.text::text').extract_first() ,\n",
    "            'autor':response.css('small.author::text').extract_first() ,\n",
    "            'categorias':response.css('a.tag::text').extract() , \n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
