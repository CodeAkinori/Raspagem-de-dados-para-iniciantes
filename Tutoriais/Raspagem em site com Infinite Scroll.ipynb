{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.creativecommons.org/l/by/4.0/88x31.png)\n",
    "# Raspagem em site com Infinite Scroll\n",
    "\n",
    "Agora vamos aprender como lidar com sites que utilizam **infinite scroll**. Infinite Scroll é uma técnica usada em muitos sites para ficar carregando sempre informação na tela do usuário, assim não vai existir um botão na tela chamado `next` para passar as páginas, pois não existirão páginas.\n",
    "\n",
    "O nosso site de teste será [uma versão do quotes](http://quotes.toscrape.com/scroll). Sempre que rolarmos para baixo, o site carrega mais informação pra gente, mas como isso acontece?\n",
    "\n",
    "## Hora de inspecionar!\n",
    "\n",
    "Dessa vez não iremos olhar o código, ainda, vamos olhar a tabela chamada *\"networking\"*. Coloque nessa tabela e desça a página até as informações serem atualizadas. Ao descer a página o browser faz um novo request para o servidor, que retorna um arquivo .json onde contém os arquivos. Sendo assim, a informação ja está estruturada para a gente, nós apenas precisamos salva-lá em um arquivo. Então vamos fazer o request com a nossa Spider e extrair o arquivo.\n",
    "\n",
    "Dentro da tabela **`Network`** existe uma tabela chamada **`headers`**, dentro dela vai existir um link url de onde veio nosso arquivo .json, vamos usar isso na nossa Spider.\n",
    "\n",
    "## Hora de codar\n",
    "\n",
    "* Vamos ver o que vem dentro desse url que pegamos. Para isso vamos abrir o shell do scrapy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy shell http://quotes.toscrape.com/api/quotes?page=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* Agora vamos criar nossa spider, digite no terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy genspider infinite_scroll quotes.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seu código vai estar assim:\n",
    "```Python\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class InfiniteScrollSpider(scrapy.Spider):\n",
    "    name = 'infinite_scroll'\n",
    "    allowed_domains = ['quotes.com']\n",
    "    start_urls = ['http://quotes.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass\n",
    "```\n",
    "\n",
    "* No `start_urls` vamos colocar o link q pegamos no `headers`: \n",
    "```python \n",
    "    start_urls = ['http://quotes.toscrape.com/api/quotes?page=1']\n",
    "```\n",
    "* Agora vamos ver o que conseguimos extrair desse site, no shell do Scrapy, rode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aqui esta nosso .json, guardar essas informações em uma variável, assim:\n",
    "```Python\n",
    "dados = json.loads(response.text) \n",
    "```\n",
    "\n",
    "* Agora devemos dividir isso em um dicionário para salva. Olhando o arquivo .json percebemos que ele esta dividido em `author_name`, `text` e `tags`. Vamos criar um `yield` para guardar essas informações. Isso tudo dentro de um *`for`*, assim ele irá percorrer todas as frases da página e repetir o processo.\n",
    "```Python\n",
    "    for frase in dados['quotes']:\n",
    "            yield {\n",
    "                'name_author': frase['author']['name'],\n",
    "                'texto': frase['text'],\n",
    "                'categorias': frase['tags'],\n",
    "            }\n",
    "```\n",
    "***Traduzi as divições, mas possuem a mesma funcionalidade.***\n",
    "\n",
    "* Agora ele acessa os *`Quote`*s, extraindo as informações de acordo com suas categorias e salvando no nosso dicionário, mas agora precisamos acessar as demais páginas.\n",
    "* Ainda dentro da função *`parse`*, usando o *`has_next`* nos dados vamos checar se existe na página antes de prosserguimos. É só colocar isso dentro de um if, vai ficar assim:\n",
    "```Python\n",
    "    if dados['has_next']:\n",
    "```\n",
    "* Agora vamos prosseguir para as próximas páginas.\n",
    "```Python\n",
    "    if dados['has_next']:\n",
    "        proxima_pag = dados['page'] + 1\n",
    "        yield scrapy.Request(url=self.api_url.format(proxima_pag), callback=self.parse)\n",
    "```\n",
    "* Pronto, agora nosso código é capaz de extrair todas as informações dessa API e guardar no seu próprio dicionário.\n",
    "\n",
    "## Código final:\n",
    "```Python\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "import json\n",
    "\n",
    "class InfiniteScrollSpider(scrapy.Spider):\n",
    "    name = 'infinite_scroll'\n",
    "    allowed_domains = ['quote.com']\n",
    "    start_urls = ['http://quotes.toscrape.com/api/quotes?page=1']\n",
    "\n",
    "    def parse(self, response):\n",
    "        dados = json.loads(response.text)\n",
    "        for frase in dados['quotes']:\n",
    "            yield {\n",
    "                'name_author': frase['author']['name'],\n",
    "                'texto': frase['text'],\n",
    "                'categorias': frase['tags'],\n",
    "            }\n",
    "\n",
    "        if dados['has_next']:\n",
    "            proxima_pag = dados['page'] + 1\n",
    "            yield scrapy.Request(url=self.api_url.format(proxima_pag), callback=self.parse)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
